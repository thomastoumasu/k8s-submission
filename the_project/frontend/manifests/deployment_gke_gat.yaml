apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-dep
  namespace: project
spec:
  replicas: 1
  selector:
    matchLabels:
      app: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: frontend
    spec:
      volumes:
        - name: shared-image
          persistentVolumeClaim:
            claimName: image-claim
      containers:
        - name: frontend
          # image: thomastoumasu/k8s-frontend:3.3-amd # try with localhost, does not seem to work
          # image: thomastoumasu/k8s-frontend:3.3b-amd # first copy IP in Dockerfile
          image: PROJECT/FRONTEND_IMAGE
          imagePullPolicy: Always
          volumeMounts:
            - name: shared-image
              mountPath: /usr/share/nginx/html/shared
          resources:
            requests: # kube-scheduler uses this information to decide which node to place the Pod on and reserves at least this amount specifically for that container to use
                memory: "64Mi" # in bytes, i for power of 2
                cpu: "250m" # 1 CPU unit is equivalent to 1 physical CPU core, or 1 virtual core, depending on whether the node is a physical host or a virtual machine running inside a physical machine.
            limits: # kubelet enforces the limit so that the running container is not allowed to use more of that resource:
              memory: "128Mi" # Above this, the kernel may terminate the container (OOMKilled, Out Of Memory) when it detects memory pressure. 
              cpu: "500m" # Around this, the kernel will restrict access to the CPU (CPU throttling).
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-svc
  namespace: project
spec:
  type: ClusterIP # for gateway in gke
  selector:
    app: frontend
  ports: # will let TCP traffic from port 1234 to port 80
    - port: 1234
      protocol: TCP
      targetPort: 80